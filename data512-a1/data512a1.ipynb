{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"data512a1.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":[],"authorship_tag":"ABX9TyPIi6d/XRIShg7qzW6KFWLi"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"1jOAHcibbwCc"},"source":["***English Wikipedia page views, 2008 - 2017***\n","\n","Data Curation Assignment (DATA512 A1) by Avani Bajaj (avanib01@uw.edu)\n","\n","The goal of this assignment is to acquire, process, and analyze a dataset of monthly traffic on English Wikipedia from December 2007 to August 2020, inclusively. The steps taken are meant to be fully reproducible."]},{"cell_type":"markdown","metadata":{"id":"mdc3viB6csD3"},"source":["Import Packages"]},{"cell_type":"code","metadata":{"id":"rzscbrzWStln"},"source":["import requests\n","import csv\n","import pandas as pd\n","import numpy as np\n","import json\n","from datetime import datetime, timedelta\n","import matplotlib.pyplot as plt\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WXv403ypcyrt"},"source":["***Step 1 : Data Acquisition*** \n","\n","Setting API endpoints for the pagecount API and pageviews API"]},{"cell_type":"code","metadata":{"id":"T7u7mPOYSyCB"},"source":["count_endpoint = 'https://wikimedia.org/api/rest_v1/metrics/legacy/pagecounts/aggregate/{project}/{access}/{granularity}/{start}/{end}'\n","view_endpoint = 'https://wikimedia.org/api/rest_v1/metrics/pageviews/aggregate/{project}/{access}/{agent}/{granularity}/{start}/{end}'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fiETemTjdBCR"},"source":["Creating parameters to call the API"]},{"cell_type":"code","metadata":{"id":"A8uh7dbwSyDn"},"source":["params_counts_allsites = {'project' : 'en.wikipedia.org',\n","            'access' : 'all-sites',\n","            'granularity' : 'monthly',\n","            'start' : '2008010100',\n","            'end' : '2020070100'\n","            }\n","\n","params_counts_mobilesites = {'project' : 'en.wikipedia.org',\n","            'access' : 'mobile-site',\n","            'granularity' : 'monthly',\n","            'start' : '2008010100',\n","            'end' : '2020070100'\n","            }\n","    \n","params_counts_desktopsites = {'project' : 'en.wikipedia.org',\n","            'access' : 'desktop-site',\n","            'granularity' : 'monthly',\n","            'start' : '2008010100',\n","            'end' : '2020070100'\n","            }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3pCjPmQ2SyGK"},"source":["params_views_desktop = {'project' : 'en.wikipedia.org',\n","            'access' : 'desktop',\n","            'agent' : 'user',\n","            'granularity' : 'monthly',\n","            'start' : '2015070100',\n","            'end' : '2020070100'\n","            }\n","\n","params_views_mobileweb = {'project' : 'en.wikipedia.org',\n","            'access' : 'mobile-web',\n","            'agent' : 'user',\n","            'granularity' : 'monthly',\n","            'start' : '2015070100',\n","            'end' : '2020070100'\n","            }\n","\n","params_views_mobileapp = {'project' : 'en.wikipedia.org',\n","            'access' : 'mobile-app',\n","            'agent' : 'user',\n","            'granularity' : 'monthly',\n","            'start' : '2015070100',\n","            'end' : '2020070100'\n","            }"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Gxu5RgY4dLKa"},"source":["API calls to get mobile, desktop and both Wikipedia traffic data. The output is saved as json file with the format that includes the start and end date"]},{"cell_type":"code","metadata":{"id":"tdo5sy74SyIr"},"source":["api_call = requests.get(count_endpoint.format(**params_counts_allsites))\n","response_counts_allsites =api_call.json()\n","with open('pagecounts_all-sites_200801-202008.json', 'w') as f:\n","    json.dump(response_counts_allsites, f)\n","    \n","api_call = requests.get(count_endpoint.format(**params_counts_mobilesites))\n","response_counts_mobilesite = api_call.json()\n","with open('pagecounts_mobile-site_200801-202008.json', 'w') as f:\n","    json.dump(response_counts_mobilesite, f)\n","\n","\n","api_call = requests.get(count_endpoint.format(**params_counts_desktopsites))\n","response_counts_desktopsite = api_call.json()\n","with open('pagecounts_desktop-site_200801-202008.json', 'w') as f:\n","    json.dump(response_counts_desktopsite, f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zUR7v4LCSyLh"},"source":["api_call = requests.get(view_endpoint.format(**params_views_desktop))\n","response_views_desktop = api_call.json()\n","with open('pageviews_all-sites_201507-202008.json', 'w') as f:\n","    json.dump(response_views_desktop, f)\n","    \n","api_call = requests.get(view_endpoint.format(**params_views_mobileweb))\n","response_views_mobileweb = api_call.json()\n","with open('pageviews_mobile-web_201507-202008.json', 'w') as f:\n","    json.dump(response_views_mobileweb, f)\n","\n","api_call = requests.get(view_endpoint.format(**params_views_mobileapp))\n","response_views_mobileapp = api_call.json()\n","with open('pageviews_mobile-app_201507-202008.json', 'w') as f:\n","    json.dump(response_views_mobileapp, f)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I7TRUnZWdnwi"},"source":["***Step 2: Data Processing***\n","\n","Perform data pre-processing on pagecount and pageviews"]},{"cell_type":"code","metadata":{"id":"pCnl4_UZSyOn"},"source":["# get items and convert the responses into dataframes. This is required in for loop to get the length of the file\n","resp_count_df1 = pd.DataFrame(response_counts_allsites['items'])\n","resp_count_df2 = pd.DataFrame(response_counts_mobilesite['items'])\n","resp_count_df3 = pd.DataFrame(response_counts_desktopsite['items'])\n","\n","# get items and convert the responses into dictionary to access individual objects\n","resp_count_dict1 = response_counts_allsites['items']\n","resp_count_dict2 = response_counts_mobilesite['items']\n","resp_count_dict3 = response_counts_desktopsite['items']\n","\n","# create empty list\n","resp_count_all_view = []\n","resp_count_time = []\n","resp_count_mobile_view = []\n","resp_count_desktop_view = []\n","\n","#load data from responses into the list. Used a single for loop to load mobile, desktop and both data \n","for x in range(0,len(resp_count_df1)):\n","    resp_count_all_view.append(resp_count_dict1[x]['count'])\n","    resp_count_time.append(resp_count_dict1[x]['timestamp'])\n","    resp_count_desktop_view.append(resp_count_dict3[x]['count'])\n","    resp_count_mobile_view.append(resp_count_dict1[x]['count']-resp_count_dict3[x]['count'])\n","   \n","# wtite the list in a single dataframe\n","pagecount = pd.DataFrame(\n","    {'Date':resp_count_time,\n","     'pagecount_all_views': resp_count_all_view,\n","     'pagecount_desktop_views': resp_count_desktop_view,\n","     'pagecount_mobile_views': resp_count_mobile_view\n","    })"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6GLWWL45SyR5"},"source":["# get items and convert the responses into dataframes. This is required in for loop to get the length of the file\n","resp_view_df1 = pd.DataFrame(response_views_desktop['items'])\n","resp_view_df2 = pd.DataFrame(response_views_mobileweb['items'])\n","resp_view_df3 = pd.DataFrame(response_views_mobileapp['items'])\n","\n","# get items and convert the responses into dictionary to access individual objects\n","resp_view_dict1 = response_views_desktop['items']\n","resp_view_dict2 = response_views_mobileweb['items']\n","resp_view_dict3 = response_views_mobileapp['items']\n","\n","# create empty list\n","resp_view_desktop = []\n","resp_view_time = []\n","resp_view_mobile = []\n","resp_view_all = []\n","\n","#load data from responses into the list. Used a single for loop to load mobile, desktop and both data \n","for x in range(0,len(resp_view_df1)):\n","    resp_view_desktop.append(resp_view_dict1[x]['views'])\n","    resp_view_time.append(resp_view_dict1[x]['timestamp'])\n","    resp_view_mobile.append(resp_view_dict2[x]['views']+resp_view_dict3[x]['views'])\n","    resp_view_all.append(resp_view_dict1[x]['views']+resp_view_dict2[x]['views']+resp_view_dict3[x]['views'])\n","    \n","# write the list in a single dataframe    \n","pageview = pd.DataFrame(\n","    {'Date':resp_view_time,\n","     'pageview_all_views': resp_view_all,\n","     'pageview_desktop_views': resp_view_desktop,\n","     'pageview_mobile_views': resp_view_mobile\n","    })"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CSZC5afSeMJx"},"source":["Merging data to get a single dataframe for analysis"]},{"cell_type":"code","metadata":{"id":"nxLsDkp9SyVM"},"source":["final_csv_df = pagecount.merge(pageview, left_on='Date', right_on='Date', how='outer').fillna(0)\n","final_csv_df['year'] = final_csv_df['Date'].str[0:4]\n","final_csv_df['month'] = final_csv_df['Date'].str[4:6]\n","final_csv_df['timestamp'] = final_csv_df['Date'].str[0:8]\n","final_csv_df['timestamp'] = final_csv_df['timestamp'].apply(pd.to_datetime)\n","final_csv_df = final_csv_df[['year', 'month', 'timestamp','pagecount_all_views', 'pagecount_desktop_views', \n","'pagecount_mobile_views','pageview_all_views','pageview_desktop_views','pageview_mobile_views']]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-nL5jgFNeTR-"},"source":["Get the CSV file and store it"]},{"cell_type":"code","metadata":{"id":"zz2ctTLLTTAS"},"source":["final_csv_df.to_csv('en-wikipedia_traffic_200801-202008.csv', sep=',')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LU56Kk5ueb-O"},"source":["***Step 3: Analysis***\n","\n","Visualize the Wikipedia traffic for :\n","\n","* Mobile traffic (For counts) \n","* Mobile traffic (for views)\n","* Desktop traffic (for counts)\n","* Desktop traffic (for views)\n","* all traffic (mobile+desktop)\n"]},{"cell_type":"code","metadata":{"id":"MRUBKtLLea1D"},"source":["# Create the plot\n","fig, ax = plt.subplots(figsize=(21,7))\n","ax.plot_date(final_csv_df['timestamp'], final_csv_df['pagecount_all_views'], '--', color='black', label='total pagecount')\n","ax.plot_date(final_csv_df['timestamp'], final_csv_df['pagecount_mobile_views'], '--', color='blue', label='mobile site pagecount')\n","ax.plot_date(final_csv_df['timestamp'], final_csv_df['pagecount_desktop_views'], '--',color='green', label = 'main site pagecount')\n","\n","ax.plot_date(final_csv_df['timestamp'], final_csv_df['pageview_all_views'], '-', color='black', label='total pageviews')\n","ax.plot_date(final_csv_df['timestamp'], final_csv_df['pageview_mobile_views'], '-', color='blue', label='mobile site pageviews' )\n","ax.plot_date(final_csv_df['timestamp'], final_csv_df['pageview_desktop_views'], '-',color='green', label='main site pageviews')\n","\n","plt.title('Page Views on English Wikipedia * (10^10)', fontsize=16)\n","plt.xlabel('Time')\n","plt.ylabel('Views')\n","ax.legend(loc='upper left')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4SUKFcFCfNID"},"source":["*Formatting the plot for better visualization*  : \n","As the data with 0 values are creating noise. We change 0s to Nan s\n","Also, in the month when the sites changed - we see a drop, we can also remove that to remove the noise"]},{"cell_type":"code","metadata":{"id":"enYZCXxefnke"},"source":["final_csv_df.replace(0, np.nan, inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JV9nsf0ufvi6"},"source":["# Create the plot after changing 0 to Nan\n","fig, ax = plt.subplots(figsize=(21,7))\n","ax.plot_date(final_csv_df['timestamp'], final_csv_df['pagecount_all_views'], '--', color='black', label='total pagecount')\n","ax.plot_date(final_csv_df['timestamp'], final_csv_df['pagecount_mobile_views'], '--', color='blue', label='mobile site pagecount')\n","ax.plot_date(final_csv_df['timestamp'], final_csv_df['pagecount_desktop_views'], '--',color='green', label = 'main site pagecount')\n","\n","ax.plot_date(final_csv_df['timestamp'], final_csv_df['pageview_all_views'], '-', color='black', label='total pageviews')\n","ax.plot_date(final_csv_df['timestamp'], final_csv_df['pageview_mobile_views'], '-', color='blue', label='mobile site pageviews' )\n","ax.plot_date(final_csv_df['timestamp'], final_csv_df['pageview_desktop_views'], '-',color='green', label='main site pageviews')\n","\n","plt.title('Page Views on English Wikipedia * (10^10)', fontsize=16)\n","plt.xlabel('Time')\n","plt.ylabel('Views')\n","ax.legend(loc='upper left')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nYPuZddfgBd2"},"source":["final_csv_df['pagecount_all_views'].min()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wWdBQ8qOgGh1"},"source":["final_csv_df.timestamp[final_csv_df.pagecount_all_views==1393717189.0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zpA8zk3If2sh"},"source":["Considering data till the time the sites were not totally transferred "]},{"cell_type":"code","metadata":{"id":"DsJeDO-yf2Hf"},"source":["# Create the plot after changing 0 to Nan\n","fig, ax = plt.subplots(figsize=(21,7))\n","ax.plot_date(final_csv_df.timestamp[final_csv_df.timestamp<'2016-08'], final_csv_df.pagecount_all_views[final_csv_df.timestamp<'2016-08'], '--', color='black', label='total pagecount')\n","ax.plot_date(final_csv_df.timestamp[final_csv_df.timestamp<'2016-08'], final_csv_df.pagecount_mobile_views[final_csv_df.timestamp<'2016-08'], '--', color='blue', label='mobile site pagecount')\n","ax.plot_date(final_csv_df.timestamp[final_csv_df.timestamp<'2016-08'], final_csv_df.pagecount_desktop_views[final_csv_df.timestamp<'2016-08'], '--',color='green', label = 'main site pagecount')\n","\n","ax.plot_date(final_csv_df['timestamp'], final_csv_df['pageview_all_views'], '-', color='black', label='total pageviews')\n","ax.plot_date(final_csv_df['timestamp'], final_csv_df['pageview_mobile_views'], '-', color='blue', label='mobile site pageviews' )\n","ax.plot_date(final_csv_df['timestamp'], final_csv_df['pageview_desktop_views'], '-',color='green', label='main site pageviews')\n","\n","plt.title('Page Views on English Wikipedia * (10^10)', fontsize=16)\n","plt.xlabel('Time')\n","plt.ylabel('Views')\n","ax.legend(loc='upper left')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XWc5tPq0fB5I"},"source":["Save the visualization plot"]},{"cell_type":"code","metadata":{"id":"gBkPa38Qe-6j"},"source":["fig.savefig('PlotPageviewsWiki.png')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"baOsN3arg7j1"},"source":[""],"execution_count":null,"outputs":[]}]}